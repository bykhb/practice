import streamlit as st
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate
import operator
from typing import Annotated, Sequence, TypedDict
from langchain_core.messages import BaseMessage
import os
from pathlib import Path

# Chroma DB ê²½ë¡œ ì„¤ì •
PERSIST_DIRECTORY = Path(__file__).parent.parent / "chroma"

# ì €ì¥ëœ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
try:
    vectorstore = Chroma(
        collection_name="food-recipe",  # ìŒì‹ ë ˆì‹œí”¼ìš© ì»¬ë ‰ì…˜
        embedding_function=OpenAIEmbeddings(),
        persist_directory=str(PERSIST_DIRECTORY),
    )
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
except Exception as e:
    print(f"Error loading vector store: {e}")
    raise

class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]
    query: str
    context: str
    response: str

def should_retrieve(state: AgentState) -> dict:
    query = state["query"]
    docs = retriever.invoke(query)
    context = "\n".join([doc.page_content for doc in docs])
    return {"next": "grade_documents", "context": context}

def grade_documents(state: AgentState) -> dict:
    context = state["context"]
    if not context:
        return {"next": "rewrite_query"}
    return {"next": "generate_answer"}

def rewrite_query(state: AgentState) -> dict:
    return None

def generate_answer(state: AgentState) -> dict:
    llm = ChatOpenAI(temperature=0)
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """
ë‹¹ì‹ ì€ ìš”ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ë ˆì‹œí”¼ ê´€ë ¨ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€í•  ë•ŒëŠ” ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¥´ì„¸ìš”:
1. ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
2. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ "ì œê°€ ê°€ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤"ë¼ê³  ë§ì”€í•´ì£¼ì„¸ìš”
3. ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”
4. ë ˆì‹œí”¼ ì •ë³´ê°€ ìˆë‹¤ë©´ ë‹¨ê³„ë³„ë¡œ ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”

ì»¨í…ìŠ¤íŠ¸:
{context}
"""),
        ("human", "{query}")
    ])
    
    chain = prompt | llm
    response = chain.invoke({
        "context": state["context"],
        "query": state["query"]
    })
    
    return {"response": response.content}

# StateGraph ì„¤ì •
from langgraph.graph import StateGraph

graph = StateGraph(AgentState)
graph.add_node("should_retrieve", should_retrieve)
graph.add_node("grade_documents", grade_documents)
graph.add_node("rewrite_query", rewrite_query)
graph.add_node("generate_answer", generate_answer)

graph.set_entry_point("should_retrieve")
graph.add_edge("should_retrieve", "grade_documents")
graph.add_edge("grade_documents", "generate_answer")
graph.add_edge("grade_documents", "rewrite_query")

chain = graph.compile()

def ask_recipe(question: str):
    initial_state = AgentState(
        messages=[],
        query=question,
        context="",
        response=""
    )
    result = chain.invoke(initial_state)
    return result["response"]

def show():
    st.title("ğŸ³ ìŒì‹ ë ˆì‹œí”¼")
    st.write("ë ˆì‹œí”¼ë‚˜ ìš”ë¦¬ ë°©ë²•ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")

    # ë„ì›€ë§ ì¶”ê°€
    st.subheader("ğŸ’¡ ë„ì›€ë§")
    st.markdown("""
    - íŠ¹ì • ìš”ë¦¬ì˜ ë ˆì‹œí”¼ë¥¼ ë¬¼ì–´ë³´ì„¸ìš”
    - ì¡°ë¦¬ ë°©ë²•ì´ë‚˜ íŒì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”
    - êµ¬ì²´ì ì¸ ì§ˆë¬¸ì¼ìˆ˜ë¡ ë” ì •í™•í•œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
    """)

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "recipe_messages" not in st.session_state:
        st.session_state.recipe_messages = []

    # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
    for message in st.session_state.recipe_messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if prompt := st.chat_input("ë ˆì‹œí”¼ë‚˜ ìš”ë¦¬ ë°©ë²•ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.recipe_messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                try:
                    response = ask_recipe(prompt)
                    st.markdown(response)
                    st.session_state.recipe_messages.append({"role": "assistant", "content": response})
                except Exception as e:
                    st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

    # ëŒ€í™” ë‚´ìš© ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ëŒ€í™” ë‚´ìš© ì´ˆê¸°í™”"):
        st.session_state.recipe_messages = []
        st.rerun() 